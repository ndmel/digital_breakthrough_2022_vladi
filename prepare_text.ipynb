{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import catboost as ctb\n",
    "from catboost import CatBoostRegressor, cv, Pool\n",
    "from catboost.utils import select_threshold, get_fpr_curve, get_roc_curve\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import gensim\n",
    "from fse import IndexedList\n",
    "from fse.models import Average\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "bad_symbols_re = re.compile('[,.«»!\"#$%&\\'()*+/:;<=>?@[\\\\]^_`{|}~]')\n",
    "stopwords = stopwords.words(['russian', 'english'])\n",
    "mystem = Mystem()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data1/vovan/shared_code/\")\n",
    "import shared_utils\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('max_column', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>created</th>\n",
       "      <th>project_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>overall_worklogs</th>\n",
       "      <th>summary_plus_comments</th>\n",
       "      <th>summary_lang</th>\n",
       "      <th>summary_lang_fix</th>\n",
       "      <th>summary_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>819952</td>\n",
       "      <td>UI тесты по заказу \"Добро КейДжи\"</td>\n",
       "      <td>2019-10-01 05:57:18.000</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>1800.0</td>\n",
       "      <td>UI тесты по заказу \"Добро КейДжи\".</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>UI tests commissioned by Dobro KG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>819949</td>\n",
       "      <td>UI тесты раздела \"Профиль\"</td>\n",
       "      <td>2019-10-01 05:59:45.000</td>\n",
       "      <td>5</td>\n",
       "      <td>Приверила и приняла MR   .</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>UI тесты раздела \"Профиль\". Приверила и принял...</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>UI tests of the \"Profile\" section</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819947</td>\n",
       "      <td>UI тесты раздела \"Личный счет\"</td>\n",
       "      <td>2019-10-01 06:00:38.000</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>14400.0</td>\n",
       "      <td>UI тесты раздела \"Личный счет\".</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>UI tests of the section \"Personal account\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                            summary                  created  \\\n",
       "0  819952  UI тесты по заказу \"Добро КейДжи\"  2019-10-01 05:57:18.000   \n",
       "1  819949         UI тесты раздела \"Профиль\"  2019-10-01 05:59:45.000   \n",
       "2  819947     UI тесты раздела \"Личный счет\"  2019-10-01 06:00:38.000   \n",
       "\n",
       "   project_id                     comments  overall_worklogs  \\\n",
       "0           5                                         1800.0   \n",
       "1           5  Приверила и приняла MR   .             7200.0   \n",
       "2           5                                        14400.0   \n",
       "\n",
       "                               summary_plus_comments summary_lang  \\\n",
       "0                UI тесты по заказу \"Добро КейДжи\".            ru   \n",
       "1  UI тесты раздела \"Профиль\". Приверила и принял...           ru   \n",
       "2                   UI тесты раздела \"Личный счет\".            ru   \n",
       "\n",
       "  summary_lang_fix                          summary_translated  \n",
       "0               ru           UI tests commissioned by Dobro KG  \n",
       "1               ru           UI tests of the \"Profile\" section  \n",
       "2               ru  UI tests of the section \"Personal account\"  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv(\"./data/train_issues.csv\")\n",
    "train_comment_df = pd.read_csv(\"./data/train_comments.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_issues.csv\")\n",
    "test_comment_df = pd.read_csv(\"./data/test_comments.csv\")\n",
    "emp_df = pd.read_csv(\"./data/employees.csv\")\n",
    "solution_df = pd.read_csv(\"./data/sample_solution.csv\")\n",
    "summary_translated_df = pd.read_csv('./data/summary_translated.csv')\n",
    "\n",
    "# add comments texts\n",
    "train_comment_df['text_padded'] = train_comment_df['text'] + '. '\n",
    "test_comment_df['text_padded'] = test_comment_df['text'] + '. '\n",
    "\n",
    "train_df = train_df.merge(train_comment_df.groupby(['issue_id'], as_index = False)['text_padded'].sum()\\\n",
    "    .rename(columns = {'text_padded':'comments', 'issue_id':'id'}), on = ['id'], how = 'left', validate = '1:1')\n",
    "\n",
    "test_df = test_df.merge(test_comment_df.groupby(['issue_id'], as_index = False)['text_padded'].sum()\\\n",
    "    .rename(columns = {'text_padded':'comments', 'issue_id':'id'}), on = ['id'], how = 'left', validate = '1:1')\n",
    "\n",
    "# combine train / test\n",
    "data_df = train_df.append(test_df, ignore_index = True)[['id', 'summary', 'created', 'project_id', 'comments', 'overall_worklogs']]\n",
    "data_df['comments'].fillna('', inplace = True)\n",
    "data_df['comments'] = data_df['comments'].map(lambda x: x.replace('\\n', ' '))\n",
    "data_df['comments'] = data_df['comments'].map(lambda x: x.replace('\\t', ' '))\n",
    "data_df['comments'] = data_df['comments'].map(lambda x: x.replace('\\r', ' '))\n",
    "data_df['summary_plus_comments'] = data_df['summary'] + '. ' + data_df['comments']\n",
    "\n",
    "# add translation\n",
    "data_df = data_df.merge(summary_translated_df, on = ['id'], how = 'left', validate = '1:1')\n",
    "\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate preprocessed corpuses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_col in ['summary', 'summary_translated', 'summary_plus_comments']:\n",
    "    data_group_df = data_df[['id', text_col]].copy()\n",
    "    \n",
    "    for preprocess_tech in ['lemma', 'stem']:\n",
    "        filename = text_col\n",
    "        \n",
    "        # get preprocessed data\n",
    "        if preprocess_tech == 'lemma':\n",
    "            data_group_df[f'text_preprocessed'] = data_group_df[text_col]\\\n",
    "                .map(lambda x: utils._text_preprocess(x))\n",
    "            filename += '_lemma'\n",
    "            \n",
    "        elif preprocess_tech == 'stem':\n",
    "            if 'translated' not in text_col:\n",
    "                # double stemmer\n",
    "                stemmer = SnowballStemmer(language='russian')\n",
    "                data_group_df[f'text_preprocessed'] = data_group_df[text_col]\\\n",
    "                    .map(lambda x: utils._text_preprocess_stem(x, stemmer))\n",
    "                stemmer = SnowballStemmer(language='english')\n",
    "                data_group_df[f'text_preprocessed'] = data_group_df[f'text_preprocessed']\\\n",
    "                    .map(lambda x: utils._text_preprocess_stem(x, stemmer))\n",
    "            else:\n",
    "                # only english stemmer\n",
    "                stemmer = SnowballStemmer(language='english')\n",
    "                data_group_df[f'text_preprocessed'] = data_group_df[text_col]\\\n",
    "                    .map(lambda x: utils._text_preprocess_stem(x, stemmer))\n",
    "            filename += '_stem'\n",
    "        \n",
    "        \n",
    "        data_group_df[['id', f'text_preprocessed']]\\\n",
    "            .to_csv('./data/text_preprocess/' + filename + '.csv', index = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linreg val function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_cv(merge_train_df):\n",
    "\n",
    "    kfold_split = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    kfold_res = []\n",
    "    pred_ids = []\n",
    "    pred_result = []\n",
    "    for train_index, test_index in kfold_split.split(merge_train_df):\n",
    "\n",
    "        X_train, y_train = merge_train_df.iloc[train_index].drop(['id', 'overall_worklogs'], 1),  merge_train_df.iloc[train_index]['overall_worklogs']\n",
    "        X_test, y_test = merge_train_df.iloc[test_index].drop(['id', 'overall_worklogs'], 1),  merge_train_df.iloc[test_index]['overall_worklogs']\n",
    "\n",
    "        linreg = ElasticNet(alpha=0.1, l1_ratio=0.9) # LinearRegression()\n",
    "        linreg.fit(X_train, y_train)\n",
    "        preds = linreg.predict(X_test)\n",
    "        val_res = r2_score(y_test, preds)\n",
    "        kfold_res.append(val_res)\n",
    "\n",
    "    return np.mean(kfold_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate tf-idfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cv_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./data/tf-idf/summary_translated_stem_tfidf_27...</td>\n",
       "      <td>0.053534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./data/tf-idf/summary_translated_stem_tfidf_50...</td>\n",
       "      <td>0.052620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./data/tf-idf/summary_translated_stem_tfidf_10...</td>\n",
       "      <td>0.050185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./data/tf-idf/summary_translated_lemma_tfidf_5...</td>\n",
       "      <td>0.043470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./data/tf-idf/summary_translated_lemma_tfidf_1...</td>\n",
       "      <td>0.042735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  cv_result\n",
       "18  ./data/tf-idf/summary_translated_stem_tfidf_27...   0.053534\n",
       "19  ./data/tf-idf/summary_translated_stem_tfidf_50...   0.052620\n",
       "20  ./data/tf-idf/summary_translated_stem_tfidf_10...   0.050185\n",
       "12  ./data/tf-idf/summary_translated_lemma_tfidf_5...   0.043470\n",
       "13  ./data/tf-idf/summary_translated_lemma_tfidf_1...   0.042735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dir = './data/text_preprocess/'\n",
    "text_files = os.listdir(text_dir)\n",
    "\n",
    "preprocess_result = []\n",
    "for file in text_files:\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "        \n",
    "    data_group_df = pd.read_csv(text_dir + file)\n",
    "    \n",
    "    for max_features in [25, 50, 100, 150, 275, 500, 1000]:\n",
    "    \n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)#, ngram_range=(1, 3))\n",
    "        X = vectorizer.fit_transform(data_group_df['text_preprocessed'])\n",
    "        tfidf_features = ['tf_idf_' + x for x in vectorizer.get_feature_names()]\n",
    "        summary_tfidf_df = pd.DataFrame(X.todense().tolist(), columns=tfidf_features)\n",
    "        merge_df = pd.concat([data_group_df[['id']], summary_tfidf_df], axis = 1)\n",
    "        \n",
    "        # check features using linreg\n",
    "        merge_df = merge_df.merge(data_df[['id', 'overall_worklogs']], on = ['id'], how = 'left', validate = '1:1')\n",
    "        merge_train_df = merge_df[pd.notnull(merge_df['overall_worklogs'])].copy()\n",
    "        cv_result = linreg_cv(merge_train_df)\n",
    "        \n",
    "        # save \n",
    "        file_save = f'./data/tf-idf/{file.split(\".\")[0]}_tfidf_{max_features}.csv'\n",
    "        preprocess_result.append([file_save, cv_result])\n",
    "        \n",
    "#         merge_df.to_csv(file_save, index = False)\n",
    "\n",
    "preprocess_result_df = pd.DataFrame(preprocess_result, columns = ['filename', 'cv_result'])\n",
    "preprocess_result_df.sort_values(['cv_result'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate bow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = './data/text_preprocess/'\n",
    "text_files = os.listdir(text_dir)\n",
    "\n",
    "# preprocess_result = []\n",
    "for file in text_files:\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "        \n",
    "    data_group_df = pd.read_csv(text_dir + file)\n",
    "    \n",
    "    for max_features in [25, 50, 100, 150, 250, 500, 1000]:\n",
    "    \n",
    "        vectorizer = CountVectorizer(max_features=max_features)\n",
    "        X = vectorizer.fit_transform(data_group_df['text_preprocessed'])\n",
    "        tfidf_features = ['bow_' + x for x in vectorizer.get_feature_names()]\n",
    "        summary_tfidf_df = pd.DataFrame(X.todense().tolist(), columns=tfidf_features)\n",
    "        merge_df = pd.concat([data_group_df[['id']], summary_tfidf_df], axis = 1)\n",
    "        \n",
    "        # check features using linreg\n",
    "        merge_df = merge_df.merge(data_df[['id', 'overall_worklogs']], on = ['id'], how = 'left', validate = '1:1')\n",
    "        merge_train_df = merge_df[pd.notnull(merge_df['overall_worklogs'])].copy()\n",
    "        cv_result = linreg_cv(merge_train_df)\n",
    "        \n",
    "        # save \n",
    "        file_save = f'./data/bow/{file.split(\".\")[0]}_bow_{max_features}.csv'\n",
    "        preprocess_result.append([file_save, cv_result])\n",
    "        \n",
    "#         merge_df.to_csv(file_save, index = False)\n",
    "\n",
    "preprocess_result_df = pd.DataFrame(preprocess_result, columns = ['filename', 'cv_result'])\n",
    "preprocess_result_df.sort_values(['cv_result'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**similarity w2v features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dir = './data/text_preprocess/'\n",
    "text_files = os.listdir(text_dir)\n",
    "\n",
    "preprocess_result = []\n",
    "for file in tqdm.tqdm(text_files, total = len(text_files)):\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "        \n",
    "    data_group_df = pd.read_csv(text_dir + file)\n",
    "    data_group_df = data_group_df.merge(data_df[['id', 'created', 'project_id', 'overall_worklogs']], how = 'left', on = ['id'], validate = '1:1')\n",
    "    \n",
    "    # train w2v model\n",
    "    summary_data = dict(zip(data_group_df['id'], data_group_df['text_preprocessed'])) \n",
    "    summary_model = utils._train_w2v_model(summary_data)\n",
    "\n",
    "    # collect similar issues time & similarity\n",
    "    result = []\n",
    "    for issue_id, group in data_group_df.groupby(['id']):\n",
    "\n",
    "        issue_created = group['created'].values[0]\n",
    "        issue_project = group['project_id'].values[0]\n",
    "\n",
    "        possible_items_dict = utils._get_possible_products_pairs_v2(issue_id, summary_model, 25)\n",
    "        possible_items_group = pd.DataFrame(possible_items_dict.keys(), columns = ['id'])\\\n",
    "            .merge(data_group_df, on = 'id', how = 'left')\n",
    "        possible_items_group['similarity'] = possible_items_dict.values()\n",
    "\n",
    "        # keep only created before main issue and from train\n",
    "        possible_items_group = possible_items_group[(possible_items_group['created'] < issue_created)\n",
    "                                                   &(pd.notnull(possible_items_group['overall_worklogs']))\n",
    "                                                   &(possible_items_group['similarity'] > 0)]\n",
    "        possible_items_group['time_since_issue'] = (pd.to_datetime(issue_created) - pd.to_datetime(possible_items_group['created'])).dt.total_seconds()\n",
    "        if possible_items_group.shape[0] > 0:\n",
    "            result_local = [issue_id]\n",
    "            for keep_n in [1, 3, 5, 25, possible_items_group.shape[0]]:\n",
    "                similar_time = np.median(possible_items_group['overall_worklogs'].values[:keep_n])\n",
    "                similar_similarity = np.median(possible_items_group['similarity'].values[:keep_n])\n",
    "                similar_time_since = np.median(possible_items_group['time_since_issue'].values[:keep_n])\n",
    "                result_local.extend([similar_time, similar_similarity, similar_time_since])\n",
    "            result.append(result_local)\n",
    "\n",
    "    # merge info\n",
    "    merge_df = pd.DataFrame(result, columns = ['id', 'sim_time_1', 'sim_sim_1', 'sim_since_1',\n",
    "                                               'sim_time_3', 'sim_sim_3', 'sim_since_3', \n",
    "                                               'sim_time_5', 'sim_sim_5', 'sim_since_5', \n",
    "                                               'sim_time_25', 'sim_sim_25', 'sim_since_25', \n",
    "                                               'sim_time_all', 'sim_sim_all', 'sim_since_all'])\n",
    "    \n",
    "    file_save = f'./data/similarity_w2v/{file.split(\".\")[0]}_basic.csv'\n",
    "    merge_df.to_csv(file_save, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**similarity intersect features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [25:49<38:14, 764.69s/it]"
     ]
    }
   ],
   "source": [
    "text_dir = './data/text_preprocess/'\n",
    "text_files = [x for x in os.listdir(text_dir) if 'comments' not in x]\n",
    "\n",
    "preprocess_result = []\n",
    "for file in tqdm.tqdm(text_files, total = len(text_files)):\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "        \n",
    "    data_group_df = pd.read_csv(text_dir + file)\n",
    "    data_group_df = data_group_df.merge(data_df[['id', 'created', 'project_id', 'overall_worklogs']], how = 'left', on = ['id'], validate = '1:1')\n",
    "    \n",
    "    # mask by create date\n",
    "    df1 = data_group_df[['id', 'text_preprocessed', 'created']].copy()\n",
    "    df1['key'] = 0\n",
    "    df2 = df1.copy()\n",
    "    df_merge = df1.merge(df2, on='key', how='outer')\n",
    "    df_merge= df_merge[(df_merge.created_x > df_merge.created_y)].copy()\n",
    "\n",
    "    # cross join\n",
    "    df_merge['summary_intersect'] = df_merge.apply(\n",
    "            lambda x: len(set(x['text_preprocessed_x'].split(' '))\\\n",
    "                  .intersection(x['text_preprocessed_y'].split(' '))), axis=1)\n",
    "\n",
    "    # add data\n",
    "    df_merge = df_merge.merge(data_group_df[['id', 'overall_worklogs', 'created']].rename(columns = {'id':'id_y'}), \n",
    "                              on = ['id_y'], how = 'left')\n",
    "    df_merge = df_merge[pd.notnull(df_merge['overall_worklogs'])]\n",
    "    df_merge = df_merge.sort_values(['id_x', 'summary_intersect'], ascending = False)\n",
    "    df_merge['text_preprocessed_x_len'] = df_merge['text_preprocessed_x'].map(lambda x: len(x.split(' ')))\n",
    "    df_merge['summary_intersect_rel'] = df_merge['summary_intersect'] / df_merge['text_preprocessed_x_len']\n",
    "    df_merge['time_since_issue'] = (pd.to_datetime(df_merge['created_x']) - pd.to_datetime(df_merge['created_y'])).dt.total_seconds()\n",
    "\n",
    "    # keep top-т\n",
    "    df_merge_top= df_merge[df_merge['summary_intersect'] > 0].groupby(['id_x']).head(25)\n",
    "    df_merge_top = df_merge_top[['id_x', 'overall_worklogs', 'summary_intersect_rel', 'time_since_issue']].rename(columns = {'id_x':'id'})\n",
    "\n",
    "    # get stats \n",
    "    result = []\n",
    "    for issue_id, possible_items_group in df_merge_top.groupby(['id']):\n",
    "        result_local = [issue_id]\n",
    "        for keep_n in [1, 3, 5, 25, possible_items_group.shape[0]]:\n",
    "            similar_time = np.median(possible_items_group['overall_worklogs'].values[:keep_n])\n",
    "            similar_similarity = np.median(possible_items_group['summary_intersect_rel'].values[:keep_n])\n",
    "            similar_time_since = np.median(possible_items_group['time_since_issue'].values[:keep_n])\n",
    "            result_local.extend([similar_time, similar_similarity, similar_time_since])\n",
    "        result.append(result_local)\n",
    "\n",
    "    # save result\n",
    "    merge_df = pd.DataFrame(result, columns = ['id', 'sim_time_1', 'sim_sim_1', 'sim_since_1',\n",
    "                                               'sim_time_3', 'sim_sim_3', 'sim_since_3', \n",
    "                                               'sim_time_5', 'sim_sim_5', 'sim_since_5', \n",
    "                                               'sim_time_25', 'sim_sim_25', 'sim_since_25', \n",
    "                                               'sim_time_all', 'sim_sim_all', 'sim_since_all'])\n",
    "\n",
    "    file_save = f'./data/similarity_w2v/{file.split(\".\")[0]}_intersect_lev.csv'\n",
    "    merge_df.to_csv(file_save, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**similarity pre-trained w2v**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [11:59<00:00, 102.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# load glove vectors\n",
    "# glove_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "\n",
    "# get data\n",
    "text_dir = './data/text_preprocess/'\n",
    "text_files = os.listdir(text_dir)\n",
    "\n",
    "preprocess_result = []\n",
    "for file in tqdm.tqdm(text_files, total = len(text_files)):\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "        \n",
    "    data_group_df = pd.read_csv(text_dir + file)\n",
    "    data_group_df = data_group_df.merge(data_df[['id', 'created', 'project_id', 'overall_worklogs']], how = 'left', on = ['id'], validate = '1:1')\n",
    "    \n",
    "    # train w2v model\n",
    "    summary_data = dict(zip(data_group_df['id'], data_group_df['text_preprocessed']))\n",
    "    words = list(map(lambda x: x.split(' '), summary_data.values()))\n",
    "    words = [[x for x in sent if x!= ''] for sent in words]\n",
    "    \n",
    "    # build a word2vec model on your dataset\n",
    "    base_model = gensim.models.Word2Vec(\n",
    "        vector_size=300, window=5, workers=1,\n",
    "        min_count=5, epochs=25, seed=42, sg=1, negative=5,)\n",
    "    base_model.build_vocab(words)\n",
    "    total_examples = base_model.corpus_count\n",
    "    \n",
    "    # add GloVe's vocabulary & weights\n",
    "    base_model.build_vocab([list(glove_vectors.key_to_index.keys())], update=True)\n",
    "\n",
    "    # get avg model\n",
    "    base_model = Average(base_model)\n",
    "    base_model.train(IndexedList(words))\n",
    "    base_model.sv.vocab = dict(zip(summary_data.keys(), base_model.sv.vectors))\n",
    "    vector_length = 300\n",
    "    summary_model = KeyedVectors(vector_length)\n",
    "    key_list = list(base_model.sv.vocab.keys())\n",
    "    vector_list = list(base_model.sv.vocab.values())\n",
    "    summary_model.add_vectors(key_list, vector_list)\n",
    "    \n",
    "    # collect similar issues time & similarity\n",
    "    result = []\n",
    "    for issue_id, group in data_group_df.groupby(['id']):\n",
    "\n",
    "        issue_created = group['created'].values[0]\n",
    "        issue_project = group['project_id'].values[0]\n",
    "\n",
    "        possible_items_dict = utils._get_possible_products_pairs_v2(issue_id, summary_model, 25)\n",
    "        possible_items_group = pd.DataFrame(possible_items_dict.keys(), columns = ['id'])\\\n",
    "            .merge(data_group_df, on = 'id', how = 'left')\n",
    "        possible_items_group['similarity'] = possible_items_dict.values()\n",
    "\n",
    "        # keep only created before main issue and from train\n",
    "        possible_items_group = possible_items_group[(possible_items_group['created'] < issue_created)\n",
    "                                                   &(pd.notnull(possible_items_group['overall_worklogs']))\n",
    "                                                   &(possible_items_group['similarity'] > 0)]\n",
    "        possible_items_group['time_since_issue'] = (pd.to_datetime(issue_created) - pd.to_datetime(possible_items_group['created'])).dt.total_seconds()\n",
    "        if possible_items_group.shape[0] > 0:\n",
    "            result_local = [issue_id]\n",
    "            for keep_n in [1, 3, 5, 25, possible_items_group.shape[0]]:\n",
    "                similar_time = np.median(possible_items_group['overall_worklogs'].values[:keep_n])\n",
    "                similar_similarity = np.median(possible_items_group['similarity'].values[:keep_n])\n",
    "                similar_time_since = np.median(possible_items_group['time_since_issue'].values[:keep_n])\n",
    "                result_local.extend([similar_time, similar_similarity, similar_time_since])\n",
    "            result.append(result_local)\n",
    "\n",
    "    # merge info\n",
    "    merge_df = pd.DataFrame(result, columns = ['id', 'sim_time_1', 'sim_sim_1', 'sim_since_1',\n",
    "                                               'sim_time_3', 'sim_sim_3', 'sim_since_3', \n",
    "                                               'sim_time_5', 'sim_sim_5', 'sim_since_5', \n",
    "                                               'sim_time_25', 'sim_sim_25', 'sim_since_25', \n",
    "                                               'sim_time_all', 'sim_sim_all', 'sim_since_all'])\n",
    "    \n",
    "    file_save = f'./data/similarity_w2v/{file.split(\".\")[0]}_w2v_pretrained.csv'\n",
    "    merge_df.to_csv(file_save, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**choose best similarity type by corr with target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>keep_n</th>\n",
       "      <th>r2_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>summary_plus_comments_lemma_w2v_pretrained.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>summary_translated_stem_intersect_lev.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>summary_stem_intersect_lev.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>summary_translated_lemma_w2v_pretrained.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>summary_lemma_w2v_pretrained.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>0.287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename keep_n  r2_train\n",
       "26  summary_plus_comments_lemma_w2v_pretrained.csv      3     0.296\n",
       "1        summary_translated_stem_intersect_lev.csv      3     0.293\n",
       "16                  summary_stem_intersect_lev.csv      3     0.289\n",
       "41     summary_translated_lemma_w2v_pretrained.csv      3     0.288\n",
       "66                summary_lemma_w2v_pretrained.csv      3     0.287"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dir = './data/similarity_w2v/'\n",
    "sim_files = os.listdir(sim_dir)\n",
    "\n",
    "preprocess_result = []\n",
    "for file in sim_files:\n",
    "    if file == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    sim_df = pd.read_csv(sim_dir + file)\n",
    "    sim_df = sim_df.merge(data_df[['id', 'overall_worklogs']], on =['id'], how = 'left', validate= '1:1')\n",
    "    \n",
    "    sim_train_df = sim_df[pd.notnull(sim_df['overall_worklogs'])].copy()\n",
    "    sim_test_df = sim_df[pd.isnull(sim_df['overall_worklogs'])].copy()\n",
    "    \n",
    "    for keep_n in ['1', '3', '5', '25', 'all']:\n",
    "        \n",
    "        sim_time_train = sim_train_df[f'sim_time_{keep_n}']\n",
    "        sim_time_test = sim_test_df[f'sim_time_{keep_n}']\n",
    "#         sim_sim_train = sim_train_df[f'sim_time_{keep_n}']\n",
    "#         sim_since_train = sim_train_df[f'sim_since_{keep_n}']\n",
    "        \n",
    "        coverage_train = np.round(sim_time_train.dropna().shape[0] / train_df.shape[0] * 100, 1)\n",
    "        coverage_test = np.round(sim_time_test.dropna().shape[0] / test_df.shape[0] * 100, 1)\n",
    "        \n",
    "        r2_train = np.round(r2_score(sim_train_df['overall_worklogs'], sim_time_train), 3)\n",
    "        corr_train = np.round(sim_train_df[['overall_worklogs', f'sim_time_{keep_n}']].corr().values[0][1], 3)\n",
    "        \n",
    "        preprocess_result.append([file, keep_n, r2_train])\n",
    "\n",
    "preprocess_result_df = pd.DataFrame(preprocess_result, columns = ['filename', 'keep_n', 'r2_train'])\n",
    "preprocess_result_df.sort_values(['r2_train'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**improving linreg funciton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053109398189843816"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_group_df = pd.read_csv('./data/bow/summary_translated_lemma_bow_100.csv')\n",
    "data_group_df = pd.read_csv('./data/tf-idf/summary_translated_stem_tfidf_250.csv')\n",
    "data_group_df = data_group_df.merge(data_df[['id', 'overall_worklogs']], on = ['id'], how = 'left', validate = '1:1')\n",
    "data_group_train_df = data_group_df[pd.notnull(data_group_df['overall_worklogs'])].copy()\n",
    "\n",
    "kfold_split = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# kfold_split = TimeSeriesSplit(n_splits=10)\n",
    "kfold_res = []\n",
    "pred_ids = []\n",
    "pred_result = []\n",
    "for train_index, test_index in kfold_split.split(data_group_train_df):\n",
    "\n",
    "    X_train, y_train = data_group_train_df.iloc[train_index].drop(['id', 'overall_worklogs'], 1),  data_group_train_df.iloc[train_index]['overall_worklogs']\n",
    "    X_test, y_test = data_group_train_df.iloc[test_index].drop(['id', 'overall_worklogs'], 1),  data_group_train_df.iloc[test_index]['overall_worklogs']\n",
    "\n",
    "    linreg = ElasticNet(alpha=0.1, l1_ratio=0.9)\n",
    "    linreg.fit(X_train, y_train)\n",
    "    preds = linreg.predict(X_test)\n",
    "    val_res = r2_score(y_test, preds)\n",
    "    kfold_res.append(val_res)\n",
    "\n",
    "np.mean(kfold_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict with linreg**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- file = './data/tf-idf/summary_translated_stem_tfidf_275.csv\n",
    "- cv = 0.053109398189843816\n",
    "- public = -0.106485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group_df = pd.read_csv('./data/tf-idf/summary_translated_stem_tfidf_275.csv')\n",
    "data_group_train_df = data_group_df[pd.notnull(data_group_df['overall_worklogs'])].copy()\n",
    "data_group_test_df = data_group_df[pd.isnull(data_group_df['overall_worklogs'])].copy()\n",
    "\n",
    "# fit\n",
    "linreg = ElasticNet(alpha=0.1, l1_ratio=0.9)\n",
    "linreg.fit(data_group_train_df.drop(['id', 'overall_worklogs'], 1), data_group_train_df['overall_worklogs'])\n",
    "\n",
    "# predict\n",
    "data_group_test_df['overall_worklogs'] = linreg.predict(data_group_test_df.drop(['id', 'overall_worklogs'], 1))\n",
    "\n",
    "# save\n",
    "save_df = solution_df.drop(['overall_worklogs'], 1)\\\n",
    "    .merge(data_group_test_df[['id', 'overall_worklogs']], on = ['id'], how = 'inner', validate= '1:1')\n",
    "assert save_df.shape[0] == solution_df.shape[0]\n",
    "assert all(save_df['id'] == solution_df['id'])\n",
    "save_df['overall_worklogs'] = save_df['overall_worklogs'].apply(lambda x: max(60, x))\n",
    "save_df.to_csv('./result/simple_submission.csv', index = False)\n",
    "save_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
